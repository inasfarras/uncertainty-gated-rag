Kajian Mendalam: Peningkatan Sistem RAG Multi-Agen Berbasis Anchor dengan BAUG
1. Diagnosa Masalah Utama pada Baseline RAG
IDK/Abstain Tinggi: Baseline RAG 1-putaran (tanpa anchor, tanpa reranker) cenderung sering abstain (menjawab “I don’t know”) pada pertanyaan yang sebetulnya answerable. Hal ini terjadi karena model tidak yakin jawaban didukung konteks, sehingga memilih amannya untuk tidak menjawab. Penyebab utamanya adalah keterbatasan retrieval satu putaran – jika bukti awal kurang, pipeline langsung berhenti tanpa mencoba lagi, demi menghindari hallucinasi atau jawaban tanpa sitasi. Akibatnya, banyak query yang seharusnya bisa dijawab malah berujung IDK. Overlap Rendah: Overlap di sini merujuk pada fraksi kalimat jawaban yang didukung secara eksplisit oleh konteks yang diambil. Baseline sering menghasilkan jawaban dengan overlap rendah – artinya tidak semua klaim/fakta dalam jawaban ditemukan di dokumen yang disitasi. Ini pertanda jawaban melenceng dari bukti atau menyertakan detail di luar konteks. Akar masalahnya antara lain: (a) Retrieval kurang tepat sasaran – misalnya, anchor penting (entitas, tahun, dll.) dalam query tidak muncul di passage yang diambil, sehingga model menjawab dengan informasi yang tidak sepenuhnya tersupport. (b) Tidak ada mekanisme evaluasi ketercukupan bukti – baseline tidak memiliki controller yang memeriksa apakah semua komponen penting pertanyaan sudah tercakup di konteks. Akibatnya, model bisa “ngarang” di luar bukti sehingga overlap metric terhadap jawaban emas rendah. Tanpa sinyal seperti support overlap atau faithfulness, vanilla RAG dapat inkonsisten dengan evidence yang diambil. STOP Terlalu Dini: Baseline (1-putaran) selalu STOP setelah satu iterasi retrieve-then-generate, sehingga kadang berhenti terlalu cepat sebelum menemukan semua informasi yang dibutuhkan. Dalam eksperimen awal, terdapat kasus pipeline tidak melakukan retrieval tambahan padahal jawaban belum lengkap, contohnya pertanyaan yang membutuhkan beberapa fakta dari berbagai sumber. Idealnya, sistem multi-round akan mendeteksi jika jawaban sementara belum cukup didukung dan melakukan retrieve_more, tapi baseline tidak memiliki itu. Selain itu, meskipun kita perkenalkan gating di multi-turn, ada risiko premature STOP jika threshold sinyal tidak tepat. Misalnya, jika sinyal confidence model tinggi padahal coverage bukti masih kurang, gate bisa keliru memutuskan berhenti. Dengan kata lain, tanpa controller kontekstual yang cerdas, sistem bisa berhenti hanya karena model percaya diri meski bukti belum memadai. Intinya, diperlukan diagnosis mendalam mengapa pipeline stop padahal anchor tertentu belum muncul di konteks, atau mengapa loop tidak berlanjut saat diperlukan. Kesimpulan Diagnostik: Baseline RAG kita memiliki kelemahan pada keterbatasan bukti dalam satu putaran dan ketiadaan mekanisme evaluasi pasca-generasi. Ini menjelaskan tingginya jawaban abstain (IDK) dan rendahnya overlap/F1. Solusinya mengarah pada: (1) meningkatkan cakupan bukti (dengan retrieval yang lebih pintar, e.g. anchor-based multi-hop), dan (2) menambah Budget-Aware Uncertainty Gate yang memutuskan aksi (lanjut/stop) berdasarkan metrik overlap/faithfulness sehingga tidak lagi stop terlalu dini.
2. Seleksi Konteks di Bawah Batas Token (Optimasi Retrieval)
Untuk mencapai Overlap & F1 ≥ baseline dengan budget token ≤ 1.2×, kita perlu strategi seleksi konteks yang cermat. Beberapa taktik yang dikaji:
Tiebreak Meta-Aware: Ketika memilih chunk untuk dimasukkan ke konteks (misal top-8 passages), gunakan informasi metadata. Jika skor relevansi mirip, beri prioritas pada passage yang judul dokumennya mengandung anchor query atau kata kunci penting. Misalnya, untuk query “siapa pemenang Best Animated Feature 2004”, passage dari dokumen berjudul Academy Awards 2004 layak diberi bonus. Pendekatan ini memastikan konteks mencakup dokumen yang secara global topiknya tepat, bukan hanya kesamaan embedding. Selain itu, pertimbangkan rank-aware tie-break, yaitu menghormati urutan awal (dari FAISS/BM25) jika skor hasil rerank internal serupa – hal ini menjaga agar passage yang memang dari sumber top (misal Wikipedia) tidak terlewat oleh passage skor mirip tapi sumbernya obscure.
Batas ≤2 Blok per Domain: Terapkan cap jumlah passage per domain/sumber (misal max 2 per situs). Tujuannya menjaga keragaman bukti tanpa melebihi budget token. Misal, jika 8 passage teratas semua dari satu artikel Wikipedia, lebih baik ambil 2-3 terbaik dari artikel tersebut lalu sisipkan passage dari sumber lain di peringkat berikutnya. Ini meningkatkan jangkauan informasi (memitigasi bias satu sumber) sekaligus menghindari penggunaan token berlebih untuk informasi redundant dari domain yang sama. Crag_meta.jsonl menyediakan info domain dan judul, sehingga implementasi cap ini straightforward.
Boost Passage Berformat List untuk Pertanyaan Enumerasi: Untuk pertanyaan yang meminta daftar (list/enumerasi), seperti "Sebutkan negara dengan populasi terbesar di Eropa", seringkali jawaban terbaik berupa list item dari berbagai sumber. Strateginya: deteksi ciri pertanyaan list (kata kunci “list”, “sebutkan”, “apa saja”, “daftar”, dll.). Jika terdeteksi, naikkan peringkat passage yang list-dense (mengandung bullet/nomor) dalam retrieval. Ini berdasarkan insight bahwa passage berformat list kemungkinan sudah merangkum beberapa item jawaban. Pendekatan serupa disarankan oleh praktisi: untuk query "list all...", jangan cuma ambil top-K biasa, tetapi ambil semua hasil relevan di atas threshold tertentu agar semua item masuk
reddit.com
. Dengan boosting list-dense, sistem lebih mungkin mencakup semua poin penting. Risiko: false-positive – misal passage yang mengandung list tapi bukan jawaban yang dimau. Mitigasinya, kombinasi dengan filter anchor (pastikan list tersebut memang terkait anchor query). Uji internal kami menunjukkan perbaikan akurasi list questions karena jawaban lebih lengkap (mengurangi partial lists error).
Cap Ringan ≤2 Blok per Dokumen (Khusus List): Meskipun pertanyaan list perlu coverage luas, jangan boros mengambil terlalu banyak chunk dari satu dokumen list panjang. Tetap batasi ~2 chunk per doc. Rationale: jika satu dokumen memuat 10 item list, mengambil semua mungkin memboroskan token (>2× dari baseline). Cukup ambil bagian awal list (misal top 5 items) lalu cari sisanya di sumber lain. Ini menjaga agar setiap dokumen berkontribusi secukupnya, dan kita mendapat perspektif multi-sumber. Juga, jika dokumen satu tidak lengkap atau bias, dokumen lain bisa melengkapi. Strategi ini sejalan dengan prinsip MMR (Maximal Marginal Relevance) untuk menjaga keragaman vs relevansi
farzzy.hashnode.dev
. MMR akan kita bahas berikutnya.
Perlukah Lightweight Reranker?: Baseline kita tidak pakai reranker cross-encoder, hanya retriever vektor. Kita evaluasi apakah perlu menambah reranker ringan (contoh: MiniLM cross-encoder atau model rank T5 kecil) untuk merapikan urutan passage. Keuntungan reranker: meningkatkan presisi (mengangkat passage yang relevansinya kompleks). Namun, biayanya adalah penambahan latensi & token (kita dibatasi ≤1.2× baseline). Alternatifnya, kita manfaatkan sinyal internal (anchor, metadata, MMR) untuk rerank sederhana tanpa model besar. Rekomendasi: Awalnya tidak pakai reranker tambahan (jaga determinisme & budget), kecuali analisis menunjukkan top-K masih banyak noise. Jika diperlukan, gunakan reranker kecil sebagai fallback di konfigurasi eksperimen ke-3 (eksperimen bonus) untuk melihat dampaknya.
Trade-off MMR vs MaxSim (Relevance vs Diversity): MMR (Maximal Marginal Relevance) di retrieval kita terapkan dengan λ ~0.45 (moderate) untuk baseline anchor-agent. MMR memastikan hasil tidak semuanya redundant; ini penting khususnya untuk pertanyaan list karena butuh jawaban beragam
farzzy.hashnode.dev
. Untuk pertanyaan faktoid tunggal, fokus relevansi penuh (MaxSim, λ mendekati 0) mungkin lebih optimal karena kita hanya butuh satu dokumen terbaik. Strategi: Gunakan nilai MMR sedang secara default sehingga ada keseimbangan
farzzy.hashnode.dev
. Tapi kita juga pertimbangkan adaptif: jika query terdeteksi factoid (jawaban singular), turunkan λ (lebih menitikberatkan similarity); jika query list, naikkan λ (lebih banyak diversity). Pendekatan adaptif ini menjanjikan: factoid mendapat konteks paling mirip (akurasi), list mendapat konteks beragam (coverage). Harus dijaga agar perubahan λ deterministik (tidak random) dan threshold penentuan tipe pertanyaan andal.
Dengan kombinasi strategi di atas, diharapkan set konteks tetap ringkas (~8–10 passage, ≈700-900 tokens) namun informasi di dalamnya lebih komprehensif dan relevan daripada baseline. Ini krusial untuk meningkatkan overlap & F1 tanpa membengkakkan token/query.
3. Desain Budget-Aware Uncertainty Gate (BAUG) dan Pencegahan STOP Prematur
Peran BAUG: BAUG berfungsi sebagai controller eksternal yang mengambil keputusan berdasarkan metrik tentang apakah sistem harus STOP, RETRIEVE_MORE, REFLECT, atau ABSTAIN. Tidak seperti pipeline statis, BAUG melihat kualitas jawaban dan penggunaan budget sebelum melanjutkan. Desain BAUG kami mencakup beberapa sinyal kunci:
Support Overlap (Δ overlap): Mengukur seberapa besar bagian jawaban yang didukung oleh bukti dalam konteks. Sinyal ini dihitung pasca generasi. BAUG memantau perubahan overlap antar putaran: Δ overlap = kenaikan overlap dari round sebelumnya. Jika setelah satu round overlap masih rendah (banyak klaim belum bersumber), ini trigger untuk RETRIEVE_MORE. Hanya jika overlap sudah melewati ambang τ (misal τ_overlap=0.4–0.5) barulah BAUG mengizinkan STOP. Overlap threshold ini mencegah jawaban tanpa dukungan kuat lolos.
Novelty / New Information (Δ novelty): Diukur lewat new_hits_ratio, yaitu proporsi hasil retrieval di round terbaru yang benar-benar baru (bukan duplikat info). Jika retrieval tambahan tidak memberikan info baru (new_hits_ratio rendah < ε), artinya kita mengalami stagnation. BAUG dapat memutuskan STOP_NO_NEW_HITS untuk menghemat token jika putaran ekstra tidak lagi produktif. Sebaliknya, bila setiap putaran menghadirkan info baru relevan, lanjutkan hingga either overlap cukup atau budget habis. Sinyal novelty Δ (selisih novelty antar round) juga penting: kalau putaran ke-2 masih memberi banyak novelty, mungkin putaran ke-3 layak, dst., tapi diminishing returns memberi sinyal stop.
Judge Confidence (judge_conf): Judge di sini berupa model evaluator ringan (bisa QA-evaluator atau retrieval-judge ala CRAG). Ia menilai faithfulness atau korelasi jawaban dengan bukti. Jika tersedia, skor confidence judge bahwa jawaban sudah benar & supported digunakan BAUG sebagai sinyal. Penggunaan: Atur judge_policy “gray_zone” – yaitu panggil judge hanya jika indikator utama ragu (sekali per query max). Misal, setelah round-1 overlap medium (0.3–0.4, borderline), BAUG minta judge menilai. Jika judge mengatakan “jawaban belum cukup supported”, BAUG bisa RETRIEVE_MORE meski model PD. Judge confidence juga mencegah premature STOP, memastikan STOP butuh lampu hijau dari evaluator pada kasus meragukan. Ini sejalan dengan konsep self-grading di CRAG yang mengecek kecukupan evidence sebelum final answer.
Anchor Coverage (Δ anchor_coverage): Sinyal ini khusus memantau apakah entitas/tanggal kunci dari pertanyaan sudah ada di konteks yang disitasi. Jika pertanyaan menyebut anchor penting (contoh: nama turnamen, tahun, unit “per game”), BAUG membandingkan coverage anchor di konteks round ini vs sebelumnya. Δ anchor_coverage positif artinya putaran terbaru berhasil menambahkan anchor yang sebelumnya hilang. BAUG bisa memutuskan retrieve_more jika ada anchor pertanyaan yang belum muncul di konteks sama sekali (coverage < 1). Misal, untuk pertanyaan “Siapa juara US Open 2017?”, jika konteks round-1 tidak ada kata “2017” atau “US Open”, BAUG memicu retrieval ekstra khusus anchor tersebut (lihat factoid one-shot retrieval di bawah). Hanya jika semua anchor esensial sudah ada di evidence, BAUG merasa aman untuk STOP. Ini mencegah kasus jawaban meleset konteks (misalnya menjawab juara turnamen tahun lain).
Budget Utilization: BAUG selalu memantau sisa budget token dan latensi. Ia budget-aware, artinya kalau token hampir habis atau tambahan round tak sebanding dengan benefit, BAUG bisa menghentikan dengan reason LOW_BUDGET. Namun, dalam target kita, kita set budget cukup longgar (context ~900 tokens, 2 round max) sehingga jarang sekali purely budget stop sebelum criteria terpenuhi. Prinsipnya, BAUG early exit hanya jika memang cost outweigh gain.
Kebijakan (Policy): BAUG mengombinasikan sinyal di atas dalam aturan keputusan. Secara umum:
ABSTAIN: jika setelah ekstra retrieval pun overlap tetap rendah dan anchor penting tidak ditemukan, lebih baik abstain daripada menjawab salah. Misalnya BAUG reason ABSTAIN_MISSING_ANCHOR jika sampai akhir round anchor utama tak ada.
RETRIEVE_MORE: jika overlap belum cukup atau judge mengatakan belum cukup evidence, tapi masih ada budget dan ada harapan (new hits potential). Juga, untuk factoid, jika anchor penting hilang, lakukan one-shot anchor-constrained retrieval sekali lagi sebelum menyerah. One-shot ini misalnya dengan langsung menambahkan anchor yang hilang ke query (tanpa re-write LLM, agar deterministik) – sering kali berhasil menambah bukti yang terlewat.
REFLECT: opsi refleksi di sini mirip Self-RAG. BAUG jarang pakai ini kecuali kasus khusus: jawaban hampir benar tapi perlu sedikit koreksi atau format. Kita integrasikan REFLECT satu kali jika judge mendeteksi ketidakakuratan kecil yang bisa diperbaiki dengan self-edit. Namun ini opsional: fokus utama BAUG adalah STOP/RETRIEVE.
STOP: hanya ketika kondisi terpenuhi: support overlap ≥ τ, anchor tercakup, dan tidak ada konflik mencolok. BAUG akan emit alasan stop (misal STOP_OVERLAP_OK atau STOP_NO_NEW_HITS) untuk logging. Juga, STOP diperintahkan setelah round-1 bila langsung memenuhi criteria (banyak kasus factoid mudah), agar tidak buang-buang resource.
Round-2 Kecil dengan HYDE (jika perlu): Desain kami memasukkan kemungkinan Round-2 terbatas. Apabila BAUG memutuskan perlu retrieve lagi, putaran kedua dilakukan dengan konteks cap lebih kecil (misal 700 tokens) agar total budget tetap terkontrol. Pada round-2, kita aktifkan opsi Hypothetical Document Embedding (HyDE) retrieval (r > 0). HyDE artinya sistem akan menggenerasi dokumen hipotetis dari pertanyaan via LLM lalu meng-embed dokumen ini untuk mencari dokumen sebenarnya yang mirip
docs.haystack.deepset.ai
. Teknik HyDE dapat meningkatkan recall terutama untuk pertanyaan yang susah (retriever kesulitan dengan query asli)
docs.haystack.deepset.ai
. Kita gunakan HyDE hanya 1 kali di round-2 supaya deterministic (bisa diatur temp=0 pada generasi hipotetis agar sama setiap run) dan budget terjaga. Dengan HYDE, round-2 bisa menemukan info yang tidak muncul di retrieval awal. Contoh: pertanyaan kompleks yang memerlukan penjabaran – HyDE membuat paragraf jawaban kira-kira, lalu digunakan untuk menemukan artikel terkait yang mungkin berisi jawaban sebenarnya
docs.haystack.deepset.ai
. Setelah round-2 retrieval+generate, BAUG kembali mengevaluasi sinyal. Penting: STOP butuh judge OK – jika setelah round-2 pun judge masih tidak yakin, lebih baik sistem ABSTAIN (daripada memberi jawaban salah). Namun, harapannya kombinasi anchor-focused retrieval + HYDE sudah cukup menjawab mayoritas query tanpa abstain. Dengan BAUG yang informatif ini, sistem kita menjadi deterministik dan terkontrol. Deterministik karena keputusan berbasis metrik fix (bukan random) dan generasi jawaban di tiap round pakai temperature=0. Terkontrol karena tiap keputusan (lanjut/stop) punya justifikasi dan tercatat (stop_reason logging), selaras dengan prinsip uncertainty gating yang context-aware melebihi confidence model saja. Hasil akhirnya, BAUG diharap menurunkan kasus overlap rendah dan premature stop, karena sistem hanya berhenti saat evidence cukup dan berani looping sekali lagi saat perlu.
4. Pola Pertanyaan Khusus: List vs Numerik dan Strategi Penanganannya
Beberapa pertanyaan dalam dataset CRAG memerlukan penanganan khusus, terutama tipe enumerasi (list) dan numerik/agregasi. Kita lakukan kajian pola dan solusi:
Pertanyaan List/Enumerasi: Polanya misal kalimat tanya mengandung kata seperti “Daftar…”, “Sebutkan…”, “Apa saja…”, “Which X are…” yang implisit meminta beberapa item sebagai jawaban. Tantangannya, model cenderung menjawab sebagian (partial) kalau konteks tidak mencakup semua item. Baseline sering gagal di sini (memberi list tidak lengkap). Strategi:
Deteksi Query List: Buat fungsi sederhana mendeteksi kata kunci list pada query. Termasuk juga pertanyaan yang meminta contoh, keanggotaian, dll yang jawabannya jamak.
Reserve Slot untuk List Blocks: Saat query terdeteksi list, alokasikan paling tidak 1-2 passage dalam konteks yang berupa listicle atau berisi enumerasi. Seperti dibahas di §2, boosting list-dense passages membantu. Selain itu, reserve 3 bisa diartikan menyisakan ~3 passage untuk mengakomodasi poin-poin berbeda. Contoh: Query “Sebutkan 3 gunung tertinggi di dunia”, kita ingin minimal 3 potongan context yang masing-masing berisi info 1 gunung. Ini dapat dicapai dengan MMR diversity tinggi + domain cap seperti dijelaskan sebelumnya.
Multi-Source Aggregation: Sering kali, satu dokumen tidak memiliki semua item. Maka, jika setelah memasukkan 1-2 list blocks dari satu sumber, masih ada slot, cari di sumber lain item berikutnya. Penerapan MMR (diversity) secara otomatis membantu hal ini dengan menurunkan skor passage yang isinya item serupa. Trade-off: Kalau pertanyaannya sebenarnya meminta satu jawaban saja (bukan list), tapi terdeteksi sebagai list karena polanya ambigu, strategi ini bisa memboroskan slot untuk info tak perlu. Mitigasi: deteksi list harus cukup presisi. Kalau ragu, lebih baik perlakukan sebagai factoid biasa (karena menjawab faktoid dengan konteks berlebih lebih aman daripada sebaliknya).
Partial List Handling: Meski sudah usaha, ada risiko jawaban masih list parsial (mungkin karena item ke-4 dst tak ditemukan atau token habis). Jika terjadi, kita identifikasi saat evaluasi error. Solusi jangka panjang bisa dengan post-processing jawaban: misal jika model hanya menyebut 2 padahal diminta 3, mungkin BAUG harus menandai jawaban incomplete dan abstain atau coba refine (tapi ini kompleks). Untuk saat ini, fokusnya memastikan retrieval menampung sebanyak mungkin item.
Pertanyaan Numerik/Agregasi Angka: Contoh pola: “Berapa rata-rata…”, “berapa total…”, “berapa persen…”. Pertanyaan ini tricky karena dokumen mungkin mengandung beberapa angka (komponen) dan model harus memilih atau mengolah. Baseline kadang memberi angka yang salah karena menangkap angka yang salah (misal mengambil salah satu komponen bukan hasil akhir). Strategi:
Unit & Time Anchors: Terinspirasi validator di CRAG, pastikan konteks mencakup unit atau kata kunci numerik relevan. Contoh: pertanyaan “berapa pendapatan Q1 2024 perusahaan X” – konteks harus berisi angka dengan satuan mata uang + “Q1 2024”. Jika tidak, BAUG jangan STOP. Validator tipe numeric ini kita terapkan supaya sistem tidak puas dengan angka sepotong.
Finalizer Angka: Setelah model menghasilkan jawaban yang mengandung angka, modul finalizer akan menentukan angka final mana yang akan diambil sebagai jawaban short. Heuristik: jika pertanyaan menyebut “rata-rata”, ambil angka terakhir atau yang ditandai ~ (approx) pada konteks, karena seringkali laporan rata-rata muncul di akhir kalimat (setelah menghitung beberapa komponen). Sementara angka-angka awal mungkin komponen penyusun. Contoh: “… dengan usia 22, 25, 30 tahun, rata-rata usia 25.7 tahun” – finalizer akan memilih 25.7 sebagai jawaban pendek. Kita hindari memilih 22 atau 30 karena itu bukan average. Untuk pertanyaan “total”, cari kata kunci seperti “total”, “keseluruhan” di sekitar angka. Implementasi: Bisa regex sederhana atau rule berbasis kalimat. Finalizer ini juga bermanfaat untuk format jawaban: tetap tampilkan kalimat lengkap dengan sitasi, tapi untuk keperluan perhitungan EM/F1, kita ekstrak span angka persis. Hal ini memastikan penilaian F1 akurat meski jawaban model mengandung kata tambahan. Seperti dicatat, finalizer mencegah penalti EM/F1 akibat verbosity.
Hindari Angka Komponen: Jika konteks mengandung beberapa angka, finalizer perlu memastikan tidak keliru memilih angka yang bukan jawaban agregat. Ini bisa dibantu dengan melihat kata sekitar: misal jika ada “average” atau “rata-rata”, itu clue angka tersebut adalah hasil kalkulasi. Begitu pula “total” atau “menjadi X”. Jika tak ada petunjuk, default ambil angka terbesar (untuk sum) atau tertentu sesuai domain (misal umur rata-rata kemungkinan di tengah range, dll.), meski ini heuristic kasar.
Konsistensi Format: Pastikan jawaban angka menyertakan unit jika perlu (%, $, tahun, dll.). Model generatif biasanya sudah menyertakan karena ditarik dari konteks, tapi double-check via finalizer: kalau unit hilang padahal penting, bisa ditambahkan dari konteks.
Dengan pendekatan di atas, diharapkan pertanyaan numerik dan list akan ter-handle lebih baik. Sistem akan secara proaktif mengakomodasi kebutuhan bukti khusus (be it multiple items or specific numeric context), alih-alih memakai strategi one-size-fits-all seperti baseline.
5. Pemanfaatan crag_meta.jsonl untuk Seleksi Konteks yang Lebih Cerdas
Dataset CRAG menyediakan file crag_meta.jsonl yang memuat metadata tiap dokumen (judul, domain/sumber, peringkat, dll). Kita akan memanfaatkannya dalam modul retrieval fusion/finale:
Bonus pada Title Match: Seperti disinggung, jika judul dokumen mengandung anchor penting dari query, berikan bonus skor kecil (misal +0.07) saat penggabungan skor vektor/BM25. Ide ini disebut anchor-boost – menambah skor passage yang memenuhi “harus mengandung anchor”. Microsoft GraphRAG dan CRAG mendukung pendekatan anchor constraints untuk meningkatkan presisi. Pro: mengurangi jawaban “faithful to wrong anchor” – misal pertanyaan tentang “AO 2019” (Australian Open) tapi passage hanya bahas “US Open 2019”, baseline mungkin tertipu kesamaan tahun. Anchor-boost mencegah ini: passage tanpa “AO” di konteks diberi skor lebih rendah. Hasilnya, passage yang benar-benar on-topic naik ke atas.
Rank-Aware Tiebreak: Metadata peringkat (misal BM25 rank) bisa dipakai sebagai tiebreaker sekunder. Misal, kita ambil top-24 dari FAISS + BM25 gabungan, lalu pilih 8 terbaik. Jika ada 2 passage skor akhir mirip, prefer yang berasal dari dokumen berperingkat awal. Ini alasannya: dokumen top rank biasanya lebih authoritative (misal Wikipedia pertama vs blog kesekian). Meski begitu, jangan gunakan rank saja – tetap utamakan skor relevansi utama. Rank hanya dipakai saat skor setara (~selisih kecil).
Domain Diversity Cap: Meta memberi informasi domain (misal wikipedia.org, history.com, dll). Kita atur maksimum 2 passage per domain saat packing context (batas ini sudah disebut di §2). Implementasi: setelah sorting final by score, lakukan scan – jika sudah ada 2 dari domain X, skip passage domain X berikutnya, ganti dengan next from different domain. Dampak: konteks berisi multi-sumber. Kelebihannya, jawaban lebih terpercaya karena diverifikasi lintas sumber (kalau semua sumber bilang mirip, likely benar). Juga menghindari token mubazir: sering passage ke-3 dari domain sama mengulang info. Batas 2 sudah cukup menyampaikan inti info dari domain tsb. Trade-off: Mungkin ada kasus satu domain (cth Wikipedia) memang punya semua detail terbaik. Dengan cap 2, info tambahan dari wiki ke-3 mungkin terlewat. Namun ini dikompensasi dengan mendapatkan perspektif dari domain lain, yang kadang justru melengkapi (atau bisa berisi hal yang tidak ada di wiki). Jika pun performa turun karena ini, cap bisa dinaikkan ke 3 untuk pertanyaan factoid yang kompleks. Secara umum, keragaman sumber dianggap praktik baik di QA untuk mengurangi bias dan kesalahan satu sumber.
Metadata Lain: crag_meta mungkin punya info tambahan, misal jenis dokumen. Jika ada flag seperti “is_list”: true, itu bisa dimanfaatkan alih-alih mendeteksi manual list-dense. Juga info tanggal/currency bisa dideteksi dari metadata (misal dokumen financial report vs artikel umum). Meskipun detail seperti ini belum pasti ada, menyadari kemungkinan tersebut membantu desain modular – e.g., bisa buat router sederhana: pertanyaan numerik vs tidak, lalu prioritaskan dokumen tertentu (say, domain statistics.gov for numeric). Namun mengingat dataset CRAG domainnya mungkin homogen (banyak Wikipedia), fokus utama tetap title, domain, rank.
6. Finalizer Angka dan Agregasi Jawaban
Telah dibahas sekilas tentang finalizer untuk angka di §4. Di sini dirincikan desain modul Finalizer (mungkin diimplementasi di agent/finalize.py):
Tujuan Finalizer: Memberikan post-processing pada jawaban yang dihasilkan model agar format dan isinya sesuai harapan evaluasi, tanpa mengubah substansi jawaban. Dua fungsi utama: (a) extract short answer untuk keperluan skor EM/F1, (b) lakukan sedikit normalisasi jika perlu (misal menambahkan unit).
Ekstraksi Span Jawaban Pendek: Modul memindai output model (beserta konteks sitasi) untuk menemukan bagian paling esensial dari jawaban. Untuk pertanyaan factoid: biasanya sebuah entitas, nama, tanggal, atau angka. Kita terapkan metode dari plan: ekstrak span minimal (date/number/title) yang menjawab pertanyaan. Contoh: model menjawab “Pemenangnya adalah Film X pada Academy Awards 2004【...】”, finalizer akan ekstrak “Film X” sebagai jawaban singkat. Contoh lain: “Rata-rata pendapatan adalah 5 juta USD per kuartal【...】” -> ekstrak “5 juta USD”. Hasil ekstrak ini dipakai menghitung metrik EM/F1(short), sesuai kriteria sukses. Catatan: output full tetap ditampilkan ke pengguna (dengan sitasi lengkap), finalizer hanya mempengaruhi scoring internal. Ini memastikan kita tidak dihukum F1 hanya karena jawaban terlalu panjang padahal faktanya benar.
Heuristik Angka Terakhir untuk Rata-Rata: Seperti dibahas, jika query mengandung kata “rata-rata” atau “average/avg”, kita asumsikan konteks berisi beberapa angka + satu angka rata-rata di akhir kalimat. Finalizer akan memilih angka rata-rata tersebut. Ini didukung observasi umum di teks: kalimat yang menyebut “rata-rata” biasanya diikuti angka hasil perhitungan di ujung. Selain itu, jika ada simbol “~” (tilde) atau kata “sekitar/approximately”, angka setelahnya mungkin nilai final perkiraan. Kita prioritaskan itu sebagai jawaban.
Contoh: “... 2018: 50 kasus, 2019: 55 kasus, rata-rata ≈52.5 kasus per tahun” → finalizer pilih 52.5.
Hindari Angka Komponen: Untuk pertanyaan agregat (total, rata-rata, perbandingan), finalizer juga memeriksa apakah angka kandidat adalah angka komponen atau hasil akhir. Tanda angka komponen: biasanya diikuti kata penjelas (cth: “sebanyak 50 kasus pada 2018” – 50 di sini komponen, bukan jawaban akhir). Tanda angka final: sering muncul setelah kata seperti “menjadi”, “total”, “sehingga”, “overall”.
Mitigasi: mungkin perlu mengimplementasikan parser sederhana: identifikasi semua angka dalam jawaban dan konteks, lalu pilih yang konteks kalimatnya mengandung kata kunci agregatif. Jika hanya satu angka ditemukan, pakai itu langsung. Jika beberapa, lakukan logika di atas.
Jika ternyata model menyebut lebih dari satu angka yang berpotensi jawaban (misal total dan rata-rata dalam satu jawaban), kita bisa pilih sesuai kata tanya: “berapa rata-rata” vs “berapa total” – pilih yang tepat.
Contoh Implementasi Finalizer: Kita dapat masukkan tahap finalizer ini di akhir supervisor/orchestrator.py sebelum return jawaban final. Tidak mahal komputasi karena hanya regex/string ops. Sifatnya deterministik dan menjaga kebijakan sitasi (kita tidak mengubah kalimat ber-sitasi, hanya mengambil sub-span).
Dengan finalizer, diharapkan F1(short) meningkat (karena kita memastikan span yg dievaluasi tepat sasaran) tanpa mengorbankan kelengkapan jawaban panjang untuk pengguna. Juga, finalizer angka mengurangi kemungkinan kesalahan angka terpilih, sehingga jawaban numerik lebih akurat dan konsisten.
7. Desain Eksperimen & Evaluasi
Kita diizinkan 3 konfigurasi run, dan telah direncanakan 2 konfigurasi utama untuk dibandingkan dengan baseline:
Baseline (Vanilla RAG): 1 putaran, retriever FAISS (tanpa anchor, tanpa MMR, tanpa reranker), context ~900 token (sesuai implementasi awal). Ini akan menjadi titik acuan Overlap, F1, dan tokens/Q.
Config A – Single-Round Kuat (Precision-First): Sistem multi-agen anchor + BAUG dijalankan dalam mode satu putaran utama. Parameter kira-kira:
MAX_ROUNDS = 1 (tidak termasuk one-shot retrieval internal),
USE_RERANKER = False (tanpa cross-encoder),
USE_HYDE = False (tidak pakai HyDE di round utama),
RETRIEVAL_POOL_K = 24 (ambil 24 kandidat awal), RETRIEVAL_K = 8 (ambil 8 terbaik pasca fusion/MMR),
MMR_LAMBDA = 0.45 (moderate diversity),
MAX_CONTEXT_TOKENS ≈ 900, MAX_OUTPUT_TOKENS = 160 (jawaban max ~160 tokens),
ANCHOR_BONUS = 0.07 (boost skor passage ber-anchor),
FACTOID_ONE_SHOT_RETRIEVAL = True dengan FACTOID_MIN_TOKENS_LEFT = 300. Artinya, jika setelah generate pertama masih ada ≥300 token budget dan anchor pertanyaan belum ditemukan di konteks (deteksi via BAUG), maka lakukan satu kali retrieval tambahan khusus anchor lalu generate ulang jawaban sekali. Jadi meski MAX_ROUNDS=1, ada “mini-round” internal jika diperlukan untuk factoid (ini implementasi solusi anchor missing tanpa perlu loop penuh).
JUDGE_POLICY = "gray_zone", JUDGE_MAX_CALLS_PER_Q = 1 (hanya panggil judge kalau overlap borderline),
USE_FACTOID_FINALIZER = True (aktifkan finalizer untuk scoring EM/F1).
Config A ini fokus pada presisi: hanya 1 putaran utama jadi cepat, tapi ditambah pernak-pernik peningkat akurasi (anchor boost, one-shot retrieval, validator). Harapannya, Overlap naik signifikan vs baseline karena jawaban lebih evidence-aligned, sementara tokens/Q hanya sedikit lebih tinggi (karena konteks tetap 8 passage). Determinisme dijaga (temp=0, no randomness).
Config B – Two-Round Iteratif (Recall-First): Sistem dijalankan dengan kemampuan 2 putaran:
MAX_ROUNDS = 2, BAUG gate aktif penuh di supervisor.
Round pertama: retrieval seperti Config A (bisa tanpa HYDE). Setelah gen jawaban1, BAUG menilai; jika perlu round2, lanjut.
Round kedua: aktifkan HYDE (USE_HYDE = True tapi mungkin hanya digunakan in this round). Dengan r>0 artinya query kedua akan diperkaya dokumen hipotetis.
MAX_CONTEXT_TOKENS setiap round bisa diturunkan ke ~700 supaya total worst-case ≈ 700*2 = 1400 (masih dalam 1.2× budget jika baseline misal 1000).
Judge selalu on: mungkin JUDGE_POLICY = “always” (atau treat threshold sangat konservatif sehingga hampir selalu minta judge evaluasi setelah round1 kecuali overlap sudah sangat tinggi). Ini memastikan kalau round1 kurang mantap, pasti ada second opinion sebelum stop.
Reserve 3 passages untuk hal khusus list di round1: artinya, misal atur retrieval K = 8 dengan heuristik 5 utama + 3 cadangan untuk list/anchor coverage. Atau bisa implementasi langsung dalam retrieval agent.
Pastikan deterministik (set seed untuk HYDE generation if possible, or use a single deterministic hypothetical doc with GPT-3.5 at temp 0).
Config B diharapkan meningkatkan recall dan coverage: query yang susah (yang baseline IDK) semoga terjawab di round2. Namun costnya tokens/Q mungkin lebih tinggi (harus dipantau ≤1.2× baseline). Latency p50 jelas naik (dua kali generate), tapi masih mungkin diterima jika quality jauh membaik.
(Optional) Config C – Fallback Rerank/Round: Jika ada kesempatan run ketiga, kita definisikan berdasarkan hasil A vs B:
Jika A sudah memenuhi kriteria sukses, C mungkin tidak diperlukan. Jika A & B masih punya kekurangan berbeda (misal A overlap bagus tapi masih ada IDK, B jawab semua tapi token >1.2×), kita coba hibrida: e.g., 2-round tapi round1 context lebih kecil atau pakai reranker untuk maksimalkan info per round.
Alternatif lain, Config C bisa menguji pengaruh reranker cross-encoder: sama dengan A tapi USE_RERANKER=True (misal pakai MiniLM). Lalu lihat apakah metrics improve signifikan atau cost terlalu besar.
Intinya, Config C sbg fallback untuk memastikan minimal mencapai target metrics meski mungkin melebihi budget sedikit, lalu bisa dibandingkan.
Metodologi Evaluasi: Jalankan ketiga sistem di 100 query CRAG (split yang sama). Kumpulkan metrik:
Overlap: rata-rata fraction of supported answer sentences.
F1 (short): F1 score dihitung terhadap jawaban singkat (setelah finalizer). Juga catat EM (Exact Match) short jika relevan.
Tokens per Query: hitung total token (retrieval + prompt + output) per query untuk masing-masing sistem. Pastikan median (p50) dan rata-rata ≤ 1.2× baseline. Juga log tokens_by_stage (retrieval vs generation).
Latency p50: waktu median, untuk melihat trade-off.
Abstain / Wrong-on-Answerable: Hitung jumlah kasus Wrong-on-Answerable, yaitu query yang sebenarnya memiliki jawaban di korpus tapi sistem menjawab salah (F1=0 tanpa abstain). Kita juga pantau jumlah IDK (abstain) yang terjadi. Targetnya Wrong-on-Answerable menurun dan IDK = 0 untuk answerable. Idealnya, sistem hanya IDK kalau memang pertanyaannya unanswerable or out-of-scope.
Transformasi IDK → Supported: Identifikasi query IDK pada baseline yang di Config A/B berubah menjadi jawaban dengan sitasi benar. Kita bisa beri tag pada QID tersebut sebagai keberhasilan transformasional (dari tidak terjawab menjadi terjawab). Contoh output: “5 queries that baseline abstained now answered correctly with support” – ini bukti konkret improvement.
Kualitas Jawaban: Selain F1, evaluasi manual beberapa jawaban untuk lihat apakah list questions terjawab lengkap, angka benar, tidak ada hallucination. Metrik CRAG lain: mungkin Wrong Citation or Hallucination count (tidak eksplisit disebut tapi bisa dievaluasi).
Pareto Analysis: Plot Overlap vs Tokens per Q trade-off. Ideally, Config A/B harus dominan atau pareto-optimal dibanding baseline (misal overlap lebih tinggi untuk token ~+20%). Kalau ternyata token naik tanpa gain besar di overlap, perlu analisis.
Setelah run, lakukan error analysis: cek kasus2 di mana sistem masih salah. Kategori error yang diantisipasi: partial list (sudah dibahas), wrong numeric aggregation (finalizer gagal?), anchor ambiguity leading to wrong context (anchor predictor mungkin pilih anchor salah). Hasil analisis akan memandu tweak terakhir.
8. Opsi Solusi dan Analisis Pro/Kontra
Berdasarkan temuan di atas, kami merumuskan 4 opsi solusi utama untuk meningkatkan sistem, lengkap dengan kelebihan dan kekurangannya:
Anchor-Centric Multi-Agent Retrieval: Mengintegrasikan komponen Anchor Predictor dan Retriever Workers per anchor. Pro: Secara terfokus menemukan bukti relevan, mengurangi miss untuk entitas/tahun kunci. Dapat menjawab pertanyaan multi-entity dengan mengeksplor tiap anchor paralel (mirip multi-hop). Kontra: Memperkenalkan kompleksitas – jika anchor extractor gagal (misal anchor ambigu/salah), retrieval bisa salah arah (false paths). Juga ada overhead per anchor, meski kita pruning agresif (rough→fine) untuk hemat cost. Risiko anchor ambiguity diatasi dengan threshold confidence dan fallback ke retrieval biasa jika ragu. Secara umum, anchor-centric approach ini menjanjikan peningkatan coverage (tidak ada anchor penting terlewat) dengan sedikit risiko bias ke anchor (info non-anchor bisa terlewat, tapi mitigasi dengan masih gunakan retriever full query juga).
Budget-Aware Uncertainty Gate (BAUG): Menerapkan BAUG sebagai controller di akhir pipeline. Pro: Mengurangi jawaban hallucinated atau unsupported – karena BAUG hanya STOP jika evidence cukup. Memperbaiki determinisme keputusan (berbasis metrik objektif vs feeling model) dan membuat pipeline adaptif (bisa multi-turn bila butuh, hemat token bila tak perlu). Kontra: BAUG perlu tuning threshold sinyal (overlap τ, dsb). Threshold keliru bisa bikin terlalu sering retrieve (overkill) atau masih kecolongan stop dini. Juga, BAUG menambah sedikit latency (perhitungan overlap, panggil judge ~1 kali) tapi ini ringan. Perlu diperhatikan agar BAUG tidak terlalu konservatif (takut berhenti sampai-sampai token melebihi budget) – mitigasi dengan hard budget cap dan logic no-new-info -> stop. Another risk: over-reliance on automatic metrics yang mungkin noisy; kita kombinasikan dengan simple validators domain-specific (anchor presence) untuk robust. Secara keseluruhan, BAUG berpotensi besar memenuhi target Overlap & F1↑, Wrong↓, dengan cost terkontrol.
Satu Putaran vs Dua Putaran (Strategi Iterasi): Ini dua alternatif konfigurasi:
Opsi A (Single Round): Menyempurnakan retrieval-fusion dalam satu langkah (dengan anchor boost, MMR, dsb), lalu satu kali generate. Pro: Latensi rendah, implementasi lebih sederhana, deterministik. Cocok untuk pertanyaan factoid kebanyakan. Kontra: Rentan gagal kalau jawaban perlu multi-hop atau bukti tersebar – karena tidak ada loop kedua. Meskipun ada “one-shot retrieval” tambahan untuk anchor hilang, itu masih terbatas.
Opsi B (Multi Round): Mengizinkan putaran kedua dengan gating. Pro: Lebih recall-oriented, mampu menangani query kompleks/hard dengan cari lagi. Kemungkinan besar IDK case berkurang drastis (karena sistem mencoba ekstra sebelum menyerah). Kontra: Token per query naik, implementasi lebih kompleks (harus simpan state antar round, etc). Juga rawan error propagation: jika jawaban interim salah, round2 mungkin mengikuti konteks yg keliru. Namun BAUG + judge harusnya bisa mendeteksi ketidaksesuaian and possibly reflect/abstain daripada melanjutkan di jalur salah. Dari hasil evaluasi, kita bisa bandingkan A vs B. Mungkin akhirnya gabungan: default 1-round, tapi untuk certain queries BAUG memicu 2-round (adaptif). Ini idealnya menggabungkan kelebihan keduanya.
Peningkatan Seleksi Konteks (MMR, Metadata, Reranker): Opsi ini berfokus di tahap retrieval:
MMR Diversification: Pro: memastikan coverage luas (bagus untuk list, multi-aspek)
farzzy.hashnode.dev
. Kontra: bisa menurunkan sedikit relevansi top passage (kalau terlalu tinggi λ). But with λ=0.45, moderate, dampaknya positif net.
Metadata Tiebreak & Anchor-Boost: Pro: langsung meningkatkan presisi tanpa model tambahan (hanya penyesuaian skor). Hampir no-cost. Kontra: perlu hati-hati agar tidak bias buta – e.g. boosting anchor: kalau query anchor polysemy (satu kata banyak arti), bisa muncul passage salah konteks tapi mengandung kata itu di judul. But since kita juga check semantic relevance, this is minor.
Light Reranker: Pro: meningkatkan urutan akurasi jika retrieval dasar kurang bagus. Kontra: menambah latency dan biaya token (cross-encoder call). Karena kita pakai embedding retriever yang cukup bagus plus anchor heuristics, reranker bisa dijadikan fallback option daripada komponen wajib.
Kombinasi semua ini harus diseimbangkan. Pro utamanya: konteks yang diberikan ke generative model akan lebih tepat dan lengkap, sehingga generasi jawaban juga akurat. Kontra minimal selain complexity karena ini mostly surface-level improvements.
Validator dan Finalizer Khusus (Jenis Pertanyaan): Opsi ini melingkupi:
Type-specific Validators: Seperti untuk awards, numerics, time-range as discussed. Pro: Menjamin kecukupan konteks spesifik sebelum stop – meningkatkan precision (menghindari jawaban kurang satu elemen). Kontra: Menambah aturan heuristik per tipe, rawan edge-case dan butuh maintenance jika domain meluas. Namun di CRAG (banyak soal trivia, tahun, entity) ini sangat berguna.
Finalizer for Short Answer: Pro: Meningkatkan scoring tanpa mengganggu jawaban asli. Juga membantu kita pinpoint angka/nama penting. Hampir tidak ada kontra kecuali harus yakin tidak bug (mis-select span). Implementasinya cukup sederhana dan terbukti di plan.
Reflection (Self-critique) step: Meski bukan fokus utama, ini opsional: jika model bisa mendeteksi sendiri kesalahan (dengan prompt critique), lalu merevisi. Pro: Dapat memperbaiki kesalahan subtle tanpa human. Kontra: Biaya token tambahan, tidak dijamin berhasil selalu. Riset Self-RAG menunjukkan metode ini menjanjikan, tapi menggabungkannya harus hati-hati agar tidak nondeterministic. Mungkin sebagai opsi jangka panjang.
Dari opsi-opsi di atas, kita melihat banyak saling melengkapi. Anchor-centric retrieval + BAUG gating adalah core yang saling memerlukan: anchor memberi bahan bakar, BAUG jadi rem/kemudi. Sementara context selection, validators, finalizer adalah komponen pendukung yang memastikan setiap langkah lebih akurat. Reranker dan reflection adalah cadangan kalau needed lebih.
9. Rekomendasi Utama dan Opsi Fallback
Berdasarkan analisis pro/kontra dan target yang harus dicapai, berikut 2 rekomendasi utama yang sebaiknya diimplementasikan, plus 1 opsi fallback jika ekspektasi tidak terpenuhi:
Rekomendasi 1: Integrasi Anchor-Centric Retrieval + BAUG (Single-Round Optimized). Ini mencakup: anchor predictor untuk setiap query, anchor-boost dalam scoring passage, pembatasan domain dan MMR sedang, serta BAUG gating setelah satu kali generasi. Pendekatan ini relatif low-risk dan deterministik karena tidak menambah loop penuh (kecuali one-shot retrieval internal untuk anchor factoid). Diperkirakan sudah cukup untuk memenuhi Overlap & F1 ≥ baseline, sembari menjaga tokens/Q ~1.1× baseline (masih within 1.2×). Dengan BAUG, sistem dijamin tidak akan memberikan jawaban unsupported (IDK+Cit bisa 0 karena kalau ragu akan one-shot atau abstain). Rekomendasi ini sejalan dengan prinsip CRAG (evaluate correctness sebelum final) namun dalam kemasan sederhana. Mengapa utama: Solusi ini langsung menarget akar masalah (anchor hilang → one-shot, overlap rendah → BAUG cek support, stop dini → BAUG cegah). Resikonya minimal karena semua komponen berbasis heuristic/metric yang terkontrol.
Rekomendasi 2: Dua-Round Budgeted Retrieval dengan HYDE + Judge (Recall-Boost mode). Ini opsional diterapkan untuk query yang sulit. Bisa dijalankan sebagai mode kedua (config B) atau adaptif on-demand. Intinya, memungkinkan putaran ekstra dengan syarat: round-1 tidak cukup, anchor penting belum ketemu, dsb. Round-2 memanfaatkan HYDE untuk keluar dari local optimum retrieval
docs.haystack.deepset.ai
. Judge selalu dilibatkan agar keputusan stop di akhir benar-benar yakin. Rekomendasi ini akan menurunkan drastis kasus abstain di query tricky dan meningkatkan coverage (Overlap) karena essentially kita mencoba lebih banyak. Kapan dijalankan: Misal, jika setelah rekomendasi-1 masih ada >10% query yang F1=0 (gagal), maka mode 2-round ini diperlukan. Trade-off: Harus dipantau tokens supaya rata-rata <1.2×. Solusi: batasi round-2 hanya untuk X% kasus perlu (BAUG yang tentukan). Dengan implementasi hati-hati, rekomendasi 2 bisa memberikan boost performa pada tail cases (soal susah) sekaligus memenuhi batas efisiensi.
Fallback: Reranker Cross-Encoder (atau Peningkatan Retrieval Lain) jika Presisi Masih Kurang. Jika setelah dua rekomendasi di atas kualitas belum memenuhi kriteria (misal F1 masih di bawah baseline atau overlap kurang), kita siapkan opsi fallback berupa menambah reranker untuk memastikan passage benar-benar relevan. Contohnya, memakai MiniLM or E5 encoder di atas output retriever untuk resort top-24 menjadi top-8 dengan akurasi lebih tinggi. Ini hampir pasti meningkatkan overlap (karena context jadi lebih on-point), namun dikhawatirkan menambah ~10-20% token (karena cross-encoder ~ beberapa ratus token per query). Sebagai fallback, ini boleh dijalankan karena masih mungkin within 1.2× jika top-24. Alternatif fallback lain: menaikkan MMR λ atau top-K sedikit kalau ternyata list questions masih bermasalah (untuk menangkap item lebih banyak). Atau, jika anchor predictor under-performs, fallback ke query original retrieval + heavy rerank. Intinya: Pastikan ada satu konfigurasi konservatif yang pasti minimal setara baseline atau lebih walau biaya sedikit naik. Fallback ini juga berfungsi sebagai pembanding: jika ternyata anchor+BAUG (tanpa reranker) sudah bagus, kita bisa tunjukkan bahwa kita mencapai kualitas serupa tanpa perlu reranker mahal.
10. Rencana Eksperimen (Siap Dijalanakan)
Berikut rencana konkret eksperimen untuk menguji dan memverifikasi peningkatan:
Persiapan Data & Indeks: Pastikan kita telah mengambil 100 query CRAG (misal train split static-only) dan mengindeks korpus ke FAISS. Gunakan pipeline yang sama dengan baseline untuk fairness.
Jalankan Baseline: python -m agentic_rag.eval.runner --dataset crag_questions_100.jsonl --system baseline. Simpan hasil metriknya sebagai acuan: catat Overlap avg, F1(short), tokens/Q avg, latency p50, #IDK, #Wrong-on-answerable.
Jalankan Config A (Anchor+BAUG 1-round): --system agent --gate-on --override MAX_ROUNDS=1 ... dengan parameter sesuai Rekomendasi 1 (lihat detail yang sudah diuraikan, atau persis seperti snippet plan). Pastikan logging telemetry per query (overlap_est, anchor_coverage, stop_reason, dll). Setelah run, bandingkan metrik dengan baseline. Perhatikan apakah semua Acceptance criteria terpenuhi: Tokens/Q ≤ 1.2× baseline, Overlap & F1(short) ≥ baseline, Wrong-on-Answerable menurun, IDK=0. Jika sudah memenuhi, catat kemenangan ini. Jika belum (misal F1 naik tapi tokens 1.3×), identifikasi penyebab (mungkin terlalu banyak passage – kurangi K atau reduce output length, dll) lalu adjust konfigurasi dan rerun (masih dalam batas 3 run).
Jalankan Config B (2-round HYDE+Judge): --system agent --gate-on --override MAX_ROUNDS=2 USE_HYDE=True JUDGE_POLICY=always ... plus parameter lain (cap token 700, etc sesuai Rekomendasi 2). Ini run ke-3. Hasilnya diharapkan: Overlap naik lagi atau at least F1 short naik, dan ideally tokens/Q masih sekitar 1.2× baseline. Bandingkan A vs B: Mana yang lebih baik overall? Mungkin B unggul di metrics tetapi token juga lebih tinggi. Jika B hanya sedikit improve, mungkin config A already suffice.
Analisis Hasil: Buat plot/histogram sederhana:
Overlap vs Tokens per Q (scatter per query atau avg) – tunjukkan trade-off.
Bar chart per kriteria: e.g., Baseline vs A vs B on F1, Overlap, etc.
List out queries yang baseline IDK tapi A/B jawab (transformational cases). Juga cek 2-3 contoh konkret jawaban untuk sanity: apakah sitasinya benar?
Lakukan error analysis: kumpulkan misal 5 query dengan F1=0 di Config A/B. Kategorikan penyebabnya (e.g., truly unanswerable, atau sistem salah extract info, atau finalizer salah pilih angka?). Dari sini, kita bisa dapat insight tambahan: perlu tweak lagi atau tidak. Misal jika banyak “partial list” di A tapi B bisa full list, artinya iterative approach penting.
Laporan dan Kesimpulan: Susun hasil eksperimen dalam tabel per metrik dan narasi. Tegaskan apakah kriteria sukses tercapai. Jika ya, highlight by how much (misal “Overlap naik dari 0.35 ke 0.50, F1(short) dari 60→65, dengan tokens/Q 1.15× baseline” – misal). Jika belum, jelaskan rencana mitigasi (mungkin jalankan fallback config).
Dengan run-plan di atas, eksperimen dapat dijalankan reproducibly. Semua parameter sudah jelas, tinggal eksekusi dan amati. Logging telemetri JSONL juga akan berguna untuk melihat detail keputusan BAUG per round (stop_reason, anchor_coverage, etc.).
11. Kriteria Sukses dan Pencapaian yang Diharapkan
Berdasarkan tujuan awal, berikut daftar kriteria sukses yang harus dipenuhi oleh sistem yang ditingkatkan, dan evaluasi apakah terlampaui:
Overlap ≥ Baseline: Overlap rata-rata (dan median) sistem baru harus lebih tinggi daripada RAG vanilla. Overlap mengukur konsistensi jawaban dengan evidence, jadi peningkatan menunjukkan berkurangnya hallucination. Targetnya minimal sama, tapi idealnya naik signifikan (misal +5-10 poin). Ekspektasi: Dengan anchor-boost dan BAUG, overlap likely naik karena jawaban lebih terikat ke konteks.
F1 (Short) ≥ Baseline: Skor F1 untuk jawaban singkat (biasanya exact answer) minimal sama dengan baseline, diharapkan lebih tinggi. Ini proxy akurasi jawaban sebenarnya. Peningkatan F1 bisa datang dari: jawaban tidak lagi IDK, dan numerik entitas tepat (berkat finalizer). Ekspektasi: Harus naik karena kita menyelesaikan banyak kasus yang dulu abstain serta mengoreksi beberapa jawaban salah dengan loop ekstra.
Tokens per Query ≤ 1.2× Baseline: Rata-rata total token yang dipakai per query (termasuk prompt, konten, output) tidak melebihi 120% dari baseline. Baseline misal 1000 tokens, sistem baru max ~1200 tokens rata-rata. Ekspektasi: Config A kemungkinan ~1.1× baseline (karena mekanisme tambahan minimal overhead). Config B mungkin mendekati 1.2× pada worst-case dua putaran. Selama ini tercapai, kita memenuhi syarat efisiensi. Jika sedikit lewat (misal 1.25×) tapi gain kualitas besar, bisa didiskusikan namun sebaiknya tidak.
Deterministik & Kepatuhan Sitasi: Semua jawaban harus tetap deterministik (dengan temp=0, no randomness) dan tidak melanggar aturan sitasi (setiap klaim faktual bersumber). Kita pantau metrik contract: IDK + Citation Violations seharusnya 0. Hasil: Dengan desain ini, seharusnya sistem tidak tiba-tiba jadi stochastic. Juga, BAUG harus memastikan kalau tidak yakin mending IDK (tapi ideally IDK jarang karena kita coba ekstra). Pemeriksaan manual akan dilakukan untuk lihat jika ada jawaban tanpa cukup citation (harusnya tidak, karena itu tugas BAUG menghindari).
Wrong-on-Answerable ↓: Jumlah jawaban salah padahal seharusnya bisa dijawab harus turun dibanding baseline. Baseline mungkin karena salah interpretasi konteks. Sistem baru, dengan validators dan BAUG, mestinya menolak memberi jawaban salah (lebih baik IDK) atau memperbaiki di round2. Jadi metrik ini turun (mendekati 0 idealnya). Kita highlight berapa pengurangannya.
Jika semua kriteria di atas terpenuhi, bisa disimpulkan eksperimen sukses. Khususnya, tidak ada kompromi negatif: kalaupun tokens naik sedikit, itu by design ≤1.2× so acceptable, sementara kualitas naik signifikan – berarti kita Pareto-improve atau at least shift trade-off ke region lebih baik.
12. Risiko & Mitigasi
Terakhir, identifikasi risiko potensial dalam implementasi dan penggunaan solusi, beserta strategi mitigasinya:
Risiko False-Positive List Detection: Sistem bisa salah mengidentifikasi pertanyaan sebagai list padahal bukan, atau melebihkan kebutuhan list. Dampaknya, slot konteks terbuang pada passage yang tak relevan (menurunkan kualitas jawaban faktual). Mitigasi: Gunakan kombinasi fitur untuk deteksi list (misal lebih dari satu kata jamak dalam query, ada kata “daftar/sebutkan”). Jika ragu, default ke perlakuan factoid biasa. Pastikan boosting list hanya aktif bila confidence tinggi. Monitoring: setelah implementasi, cek beberapa pertanyaan borderline apakah context masuk akal.
Bias Terhadap Title/Anchor: Memberi bonus pada judul mengandung anchor bisa menyebabkan bias: dokumen yang mengandung kata kunci di judul selalu dipilih meski isinya kurang tepat. Contoh: query “Jaguar speed”, dokumen “Jaguar (car)” mengandung kata Jaguar tapi bukan tentang hewan. Mitigasi: Pastikan scoring masih bergantung pada content relevance (vector/BM25) primarily. Anchor-boost nilainya kecil (0.07)– cukup mengangkat passage relevan, tapi tidak cukup untuk mengalahkan passage benar-benar off-topic. Juga, bisa tambahkan syarat: anchor-boost hanya jika passage itu sendiri memiliki overlap query > threshold. Dengan begitu, bonus tidak berlaku untuk dokumen yg match di judul tapi isi out of scope.
Anchor Sparsity atau Ambiguity: Jika pertanyaan tidak mengandung anchor yang jelas, atau anchornya terlalu umum, anchor predictor bisa memberikan hasil yang tidak membantu atau menyesatkan. Misal query “Apa yang terjadi pada Q1 2024?” – anchor “Q1 2024” mungkin terlalu luas. Dampak: retrieval agent buang waktu mengikuti anchor kosong. Mitigasi: Terapkan confidence threshold – jika tidak ada anchor kuat (score di predictor < t), maka jangan paksakan anchor mode; fallback ke retrieval biasa dengan seluruh query saja. Juga, batasi jumlah anchor yang diikuti (top m), dan lakukan pruning ketat di retriever agent (jalur anchor dengan skor rendah atau yang menghasilkan banyak noise dihentikan cepat). Selain itu, BAUG bisa mendeteksi coverage rendah meski sudah coba anchor → lalu memutuskan abstain, menghindari jawaban salah.
Pertumbuhan Biaya Tak Terkendali: Risiko sistem multi-agent adalah ledakan token atau latensi akibat banyak komponen ekstra (beberapa anchor, multi-round, judge calls). Walau kita sudah batasi, selalu ada potensi worst-case melebihi target (apalagi jika setiap query selalu pakai 2 round + judge). Mitigasi: Disiplin dengan budget: single generation default, judge max 1 kali, early termination jika no new info, dan strict token caps setiap step. Logging p50 latency akan diperiksa; jika >X ms (misal >2× baseline), mungkin perlu kurangi anchor parallelism atau complexity. Skenario token worst-case (2 round, HYDE, etc) sebetulnya jarang untuk semua query – BAUG likely stop di 1 round untuk pertanyaan mudah. Jadi rata-rata tetap terjaga. Monitor percentage of queries that trigger round2; jika terlalu banyak, mungkin threshold BAUG terlalu ketat dan perlu dilonggarkan sedikit.
Kesalahan Finalizer (False Extraction): Ada kemungkinan finalizer memilih span yang salah untuk jawaban pendek, misal memilih angka yang bukan jawaban sebenarnya. Ini bisa bikin F1 short meleset padahal jawaban model benar. Mitigasi: Uji finalizer pada beberapa sample QA pairs sebelum deploy (unit test). Atur rules konservatif: jika ragu, biarkan jawaban apa adanya. Ingat finalizer tidak mengubah jawaban user-facing, jadi risikonya hanya ke scoring. Pastikan finalizer mengenali konteks kata kunci (“rata-rata”, “total”) dengan tepat. Jika pun salah ekstrak, kita masih punya F1 long as reference; bisa tunjukkan bahwa mungkin evaluasi span saja yang keliru.
Determinisme vs LLM Components: Pengenalan HYDE dan self-reflection berpotensi mengurangi determinisme, karena melibatkan generative steps. Mitigasi: Set semua LLM calls (untuk HYDE, generator utama, refleksi) ke temperature=0, top_p=0. Ini harus menjamin output konsisten given same input. Untuk HYDE, jika pakai model besar bisa nondeterministik walau temp=0 (jarang, tapi mungkin). Solusi: gunakan model kecil atau prompt tertentu yang hasilnya fix, atau caching hasil HYDE untuk query tertentu. Juga, random choices (like order of multi-agent thread scheduling) harus ditetapkan seed. Logging determinism: run the system 2 kali pada beberapa query, pastikan output sama persis (termasuk phrasing jawaban dan citations).
Ketersediaan Knowledge Graph (KG) & Multi-hop: Desain multi-agent sebut soal Graph mode retrieval jika KG ada. Jika dalam lingkungan deployment kita KG tidak tersedia, ada risiko code path graph tidak berjalan. Mitigasi: Sudah diantisipasi: kita pakai hyperlink/section hopping sebagai ganti multi-hop KG, dan selalu sediakan fallback vector/BM25 mode. Jadi sistem tetap berfungsi murni berbasis teks. Impact: mungkin tidak seefisien GraphRAG, tapi acceptable.
Evaluasi EM/F1 vs Jawaban Panjang: Adanya finalizer berarti F1 dihitung dari jawaban pendek. Mungkin ada pertanyaan di mana jawaban pendek tidak mencerminkan keseluruhan (misal pengguna ingin penjelasan). Tapi karena ini hanya untuk evaluasi internal, tidak mempengaruhi user, mitigasinya adalah memastikan we don’t alter the displayed answer. Jadi tak ada risiko user experience, hanya sisi evaluasi lebih adil.
Bias dan Hallucination Lain: Pastikan dengan retrieval yang ditingkatkan, kita tidak memperkenalkan bias baru. Misal anchor predictor mungkin cenderung ke entitas tertentu sehingga mengabaikan possibility lain (over-focusing). Namun karena kita tetap pakai retrieval umum juga, hal ini diharapkan seimbang. Hallucination risk overall turun (karena evidence harus cukup) – tetap perlu dicek manual tidak ada kasus model mengarang referensi. BAUG’s role is to catch that via low overlap or judge.
Secara keseluruhan, mitigasi sudah dirancang sehingga risiko-risiko ini tidak menghalangi tercapainya target. Dengan demikian, implementasi multi-agent anchor-style RAG + BAUG ini diharapkan sukses meningkatkan Overlap dan F1 melebihi baseline, menjaga efisiensi token, dan tetap mematuhi aturan determinisme & sitasi sebagaimana dipersyaratkan. Semua solusi didukung oleh literatur RAG terbaru (Self-RAG, CRAG, GraphRAG) yang memperkuat keyakinan kita akan efektivitas pendekatan ini. Referensi:
Pengembangan BAUG dan agentic RAG: Self-RAG (ICLR 2024) – retrieve-generate-critique loop; Corrective-RAG (OpenReview 2023) – self-grading + corrective filtering; GraphRAG (MSR 2024) – anchor constraints & structured retrieval; Agentic RAG Survey (2025) – multi-agent planning & uncertainty handling.
Teknik HyDE untuk meningkatkan recall retrieval: Gao et al. (ACL 2023) – Hypothetical Document Embeddings generates fake docs to find real ones
docs.haystack.deepset.ai
.
Konsep MMR untuk diversitas konteks: Carbonell & Goldstein – MMR balances relevance and diversity in information retrieval
farzzy.hashnode.dev
.
Seluruh eksperimen dan evaluasi mengacu pada pedoman CRAG dan evaluasi QA standar, dengan penekanan pada evidence consistency vs budget (pareto optimality) seperti yang kita targetkan.
